---
title: "Genotype Assignment on Fluidigm Intensity Data"
author: "Thomas Ng"
date: "March 23, 2015"
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
library(ggplot2)
library(plyr)
library(grid)
library(gridExtra)
library(dplyr)
library(reshape2)
library(MASS)
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
```

# Objective

The goals of this report is :    

* to identify, if any, merit in working directly with raw intensity file instead of absolute intensity file   
* to determine the quality of the Sturgeon data    
* to build multiple models for the problem of inferring allelic state in diploid and triploid organisms

# Data Set
Anthony provided me a total of four control fluidigm datasets of Chinook Salmon.   
Two poor cases: ` 1381840264_raw.csv, 1381961382_raw.csv` 
and two good cases: `1382089116_raw.csv, 1382100495_raw.csv`  

Eric Crandall provided me four sturgeon data set:   
`1381905043_raw.csv, 1381935339_raw.csv, 1381992037_raw.csv, 1381992302_raw.csv`   
I am interested inThe quality of the dataset.

#Descriptive Analysis 

## Preprocessing step   
  
I extracted raw intensity csv from the software and cleaned the file. See [Prepocessing Script](#preprocessing-step). As for the raw extraction step: read `BioMark - SpecialFileProcessKey Explained.pdf` (2/20/15)

## Statistic Summary

Let's first look at a good dataset.  

```{r}

## This function returns the centroid(s) of the dye intensity value by k-mean algorithm

KMeanIt <- function (file, field, n.center=2){

kmean.dye1<- ddply(file, field, function(data){
dye1.diff<- data$chDYE1-data$bgDYE1;
sort(t(kmeans(dye1.diff, n.center, iter.max=20)$centers))
})

kmean.dye2<- ddply(file, field, function(data){
dye2.diff<- data$chDYE2-data$bgDYE2;
sort(t(kmeans(dye2.diff, n.center, iter.max=20)$centers))
})

colnames(kmean.dye1)<- c(field, paste("mean", 1:n.center, sep=""))
colnames(kmean.dye2)<- c(field, paste("mean", 1:n.center, sep=""))

list(dye1 = kmean.dye1, dye2 = kmean.dye2)
}

MakeKmerPlot<- function(test.FILE, n.center) {
  sample.kmean<- KMeanIt(test.FILE, "Name", n.center=n.center)
  assay.kmean<- KMeanIt(test.FILE, "Assay", n.center=n.center)
  
  p1<-ggplot(test.FILE, aes(x=chDYE1-bgDYE1, y=Name))+geom_point(alpha=0.2)+
  ylab("Sample Name")+
  xlab("")+
  xlim(c(0,17500))
  
  p2<-ggplot(test.FILE, aes(x=chDYE1-bgDYE1, y=Assay))+geom_point(alpha=0.2)+
  ylab("Locus ID")+
  xlab("")+
  xlim(c(0,17500))
  
  
  p3<- ggplot(test.FILE, aes(x=chDYE1-bgDYE1))+
    geom_histogram(alpha=0.2, fill="grey", colour="grey", binwidth=300)+
    aes(y=..density..)+
    xlab("abs Intensity of DYE 1")+
    xlim(c(0,17500))
  
  p4<- ggplot(test.FILE, aes(x=chDYE1-bgDYE1))+
    geom_histogram(alpha=0.2, fill="grey", colour="grey", binwidth=300)+
    aes(y=..density..)+
    xlab("abs Intensity of DYE 1")+
    xlim(c(0,17500))
  
  for (i in 1:n.center) {
    p1 <- p1 +
      geom_point(data=sample.kmean$dye1, aes_string(x=paste0("mean",i), y="Name"), col=i+1, size=2.5)
    p2 <- p2 +
      geom_point(data=assay.kmean$dye1, aes_string(x=paste0("mean",i), y="Assay"), col=i+1, size=2.5)
    p3 <- p3 +
      geom_histogram(data=sample.kmean$dye1, aes_string(x=paste0("mean", i)), binwidth=300, fill="blue", alpha=0.5)
    p4 <- p4 + 
      geom_histogram(data=assay.kmean$dye1, aes_string(x=paste0("mean", i)), binwidth=300, fill="blue", alpha=0.5)
      
  }
  
  
  grid.newpage()
  gp1 <- ggplot_gtable(ggplot_build(p1))
  gp3 <- ggplot_gtable(ggplot_build(p3))
  gp3$widths <- gp1$widths

  gp2 <- ggplot_gtable(ggplot_build(p2))
  gp4 <- ggplot_gtable(ggplot_build(p4))
  gp4$widths <- gp2$widths
  
  grid.arrange(arrangeGrob(gp1,gp3, heights=c(6,1), ncol=1),arrangeGrob(gp2,gp4, heights=c(6,1), ncol=1), ncol=2)
  
  p1<-ggplot(sample.kmean$dye1, aes(x=mean2-mean1, mean3-mean1))+
    geom_point()+
    xlab("Intensity of Centroid 2 - Centroid 1 1(sample-based)")+
    ylab("Intensity of Centroid 3 - Centroid 1 (sample-based)")+
    geom_abline(slope=3, lty=2)+geom_abline(slope=2, lty=3)
  
  p2<-ggplot(assay.kmean$dye1, aes(x=mean2-mean1, mean3-mean1))+
    geom_point()+
    xlab("Intensity of Centroid 2 - centroid 1 (locus-based)")+
    ylab("Intensity of Centroid 3 - centroid 1 (locus-based)")+
    geom_abline(slope=3, lty=2)+geom_abline(slope=2, lty=3)
  
  p3<-ggplot(sample.kmean$dye1, aes(x=mean2-mean1, y=(mean3-mean1)/(mean2-mean1)))+
    geom_point()+
    xlab("Intensity of Centroid 2 - Centroid 1 (sample-based)")+
    ylab("Intensity of Centroid 3 / Centroid 2 rel to Centorid 1")
  
  p4<-ggplot(assay.kmean$dye1, aes(x=mean2-mean1, y=(mean3-mean1)/(mean2-mean1)))+
    geom_point()+
    xlab("Intensity of Centroid 2 - Centroid 1 (locus-based)")+
    ylab("Intensity of Centroid 3 / Centroid 2 rel to Centroid 1")
  
  grid.newpage()
  grid.arrange(p1,p2,p3,p4, ncol=2, nrow=2)
  
}
```

```{r, fig.width=20, fig.height=16}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
## generate all graph
## Good dateset 1
#MakeKmerPlot(test.FILE=read.csv("1382089116_raw.csv.tbl"), n.center=3)
MakeKmerPlot(test.FILE=read.csv("1382089116_raw.csv.tbl"), n.center=4)

#MakeKmerPlot(test.FILE=read.csv("1382100495_raw.csv.tbl"), n.center=3)
MakeKmerPlot(test.FILE=read.csv("1382100495_raw.csv.tbl"), n.center=4)

# poor case
MakeKmerPlot(test.FILE=read.csv("1381961382_raw.csv.tbl"), n.center=3)

# Sturgeon set 1
#MakeKmerPlot(test.FILE=read.csv("1381905043_raw.csv.tbl"), n.center=3)
MakeKmerPlot(test.FILE=read.csv("1381905043_raw.csv.tbl"), n.center=4)

# Sturgeon set 2
MakeKmerPlot(test.FILE=read.csv("1381935339_raw.csv.tbl"), n.center=4)
```
     
### Other explorations   
   
Question: Does the order of the mean sample intensity of an allelic group follow similar ordering in individual's intensity of an allelic cluster at a locus site? The reason I ask this question is to see 1) whether there exists a fixed locus-specific effect and 2) whether there are any benefits in learning the mean sample intensity value.    
   
To compare two permutation sets, I will be using the metric of the number of swapping steps it takes to bubble sort between two sets.   
   
Let $U_i$ be a set of order statistics of length $l$ with permutation index $i$. Let $S(U_i, U_j)$ be the number of swapping steps in a bubble sort it takes to go from set $U_i$ to set $U_j$. If $U_i = U_j$, $S(U_i, U_j) = 0$. If $U_j$ is in reverse order of $U_i$, $S(U_i, U_j) = l(l-1)/2$.   
   
   
```{r}   
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
test.FILE=read.csv("1382089116_raw.csv.tbl")
clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]

clean.file <- test.FILE[test.FILE$Type=="Unknown",]
clean.file$Name <- factor(clean.file$Name)
clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))
#revalue(factor(clean.file$Final), c("XX"=2, "XY"=1, "YY"=0))

# reshape the test.FILE to get the essential
# (Assay (j), Name (i), Dye1 (y_1), Dye2 (y_2), k (k_1))
core.data <- data.frame(
  assay= as.numeric(clean.file$Assay),
  name = as.numeric(clean.file$Name),
  dye1 = clean.file$chDYE1-clean.file$bgDYE1,
  dye2 = clean.file$chDYE2-clean.file$bgDYE2,
  rel.dye1 = clean.file$Allele.X.1,
  rel.dye2 = clean.file$Allele.Y.1,
  k=as.numeric(clean.file$Final))

core.data <- arrange(core.data, assay, name)

require(inline)  ## for cxxfunction()                                                       

## bubble sort c code; I modify it to return the number of steps for swapping
src = 'Rcpp::NumericVector vec = Rcpp::NumericVector(vec_in);                               
       Rcpp::NumericVector step(1);
       double tmp = 0;                                                                      
       int no_swaps;
       step[0]=0;
       while(true) {                                                                        
           no_swaps = 0;                                                                    
           for (int i = 0; i < vec.size()-1; ++i) {                                         
               if(vec[i] > vec[i+1]) {                                                      
                   no_swaps++;                                                              
                   tmp = vec[i];                                                            
                   vec[i] = vec[i+1];                                                       
                   vec[i+1] = tmp;
                   step[0]++;
               };                                                                           
           };                                                                               
           if(no_swaps == 0) break;                                                         
       };                                                                                   
       return(step);'  
bubble_sort_cpp = cxxfunction(signature(vec_in = "numeric"), body=src, plugin="Rcpp")

mean.rel.dye1 <- daply(core.data, "name", function (x){
  sapply(1:(2+1), function(y) mean(x[x$k==y,]$rel.dye1))
  })   

ReframingOrder <- function(ref, query) {
  ref <- factor(factor(ref), ref) # preserving reference order
  q.order <- as.numeric(factor(factor(query), ref)) # get the index
  return(q.order[!is.na(q.order)]) # remove any NA
}

order.mean.k1 <- order(mean.rel.dye1[,1])
order.mean.k2 <- order(mean.rel.dye1[,2])
order.mean.k3 <- order(mean.rel.dye1[,3])

bubble.dist <- sapply(1:90000, function(x) bubble_sort_cpp(sample(1:94,94)))
mean(bubble.dist)
sd(bubble.dist)

```
     
     
Question: One of the goals in this project is to know whether we can tell if two or more samples came from the same individual based on the signal intensity. 

I am going to use Euclidean distance to measure the distance between two sample points.   

Dataset: the good diploid dataset

```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
test.FILE=read.csv("1382089116_raw.csv.tbl")
clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]

clean.file <- test.FILE[test.FILE$Type=="Unknown",]
clean.file$Name <- factor(clean.file$Name)
clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))
#revalue(factor(clean.file$Final), c("XX"=2, "XY"=1, "YY"=0))

# reshape the test.FILE to get the essential
# (Assay (j), Name (i), Dye1 (y_1), Dye2 (y_2), k (k_1))
core.data <- data.frame(
  assay= as.numeric(clean.file$Assay),
  name = as.numeric(clean.file$Name),
  dye1 = clean.file$chDYE1-clean.file$bgDYE1,
  dye2 = clean.file$chDYE2-clean.file$bgDYE2,
  rel.dye1 = clean.file$Allele.X.1,
  rel.dye2 = clean.file$Allele.Y.1,
  k=as.numeric(clean.file$Final))

core.data <- arrange(core.data, assay, name)


## consider the absolute intensity value
dist.matrix <- dlply(core.data, .(assay), function(x) {
  delta.x <- outer(x$dye1, x$dye1, "-")
  delta.y <- outer(x$dye2, x$dye2, "-")
  dist <- sqrt(delta.x^2 + delta.x^2)
  })

#for now; let's keep track of the last 1/3 of the farthest neighbours 
frac.cutoff <- 1/3
num.sample <- 94
num.assay <- 96
num.far.neig <- round(frac.cutoff * 94)


neigb.farout <- sapply(1:num.sample, function(i) {
  rowSums(
    sapply(dist.matrix, function(assay){
      a <- rep(0, num.sample)
      a[order(assay[i,])[(num.far.neig+1):num.sample]] <- 1
      a } )
    )
})

ggplot(melt(neigb.farout), aes(x=Var1, y=Var2, color=value))+geom_point()
table(melt(neigb.farout)$value)

## if we work in rel intensity value 
dist.matrix <- dlply(core.data, .(assay), function(x) {
  delta.x <- outer(x$rel.dye1, x$rel.dye1, "-")
  delta.y <- outer(x$rel.dye2, x$rel.dye2, "-")
  dist <- sqrt(delta.x^2 + delta.x^2)
  })

#for now; let's keep track of the last 1/3 of the farthest neighbours 
frac.cutoff <- 1/3
num.sample <- 94
num.assay <- 96
num.far.neig <- round(frac.cutoff * 94)


neigb.farout <- sapply(1:num.sample, function(i) {
  rowSums(
    sapply(dist.matrix, function(assay){
      a <- rep(0, num.sample)
      a[order(assay[i,])[(num.far.neig+1):num.sample]] <- 1
      a } )
    )
})

melt.neigb <- melt(neigb.farout)

ggplot(melt(neigb.farout), aes(x=Var1, y=Var2, color=round(num.assay-value, digits=-1.8)))+geom_point()
table(melt(neigb.farout)$value)


#core.data <- tbl_df(core.data)
#core.data %>% 
#  group_by() %>%
```
From this good dataset, the largest number of loci where two samples are within the distance of 2/3 of all of the neigbours is 67 out of 96.    
   
Let's examine the sturgeon dataset:

```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
test.FILE=read.csv("1381905043_raw.csv.tbl")
clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]

clean.file <- test.FILE[test.FILE$Type=="Unknown",]
clean.file$Name <- factor(clean.file$Name)
clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))
#revalue(factor(clean.file$Final), c("XX"=2, "XY"=1, "YY"=0))

# reshape the test.FILE to get the essential
# (Assay (j), Name (i), Dye1 (y_1), Dye2 (y_2), k (k_1))
core.data <- data.frame(
  assay= as.numeric(clean.file$Assay),
  name = as.numeric(clean.file$Name),
  dye1 = clean.file$chDYE1-clean.file$bgDYE1,
  dye2 = clean.file$chDYE2-clean.file$bgDYE2,
  rel.dye1 = clean.file$Allele.X.1,
  rel.dye2 = clean.file$Allele.Y.1,
  k=as.numeric(clean.file$Final))

core.data <- arrange(core.data, assay, name)
```


```{r}
g<-ggplot(core.data, aes(x=dye1,y=dye2, color=factor(k)))+
  geom_point()+
  facet_wrap(~assay, ncol=10)
g
#ggsave("abs-raw-xy-assay-sturgeonset1.pdf",g, height=20, width=30)

g<-ggplot(core.data, aes(x=rel.dye1,y=rel.dye2, color=factor(k)))+
  geom_point()+
  facet_wrap(~assay, ncol=10)
g
#ggsave("rel-raw-xy-assay-sturgeonset1.pdf",g, height=20, width=30)
```

```{r}
## consider the absolute intensity value
dist.matrix <- dlply(core.data, .(assay), function(x) {
  delta.x <- outer(x$dye1, x$dye1, "-")
  delta.y <- outer(x$dye2, x$dye2, "-")
  dist <- sqrt(delta.x^2 + delta.x^2)
  })

#for now; let's keep track of the last 1/3 of the farthest neighbours 
frac.cutoff <- 1/3
num.sample <- 94
num.assay <- 96
num.far.neig <- round(frac.cutoff * 94)

# keeps track of the farthest neighbours
neigb.farout <- sapply(1:num.sample, function(i) {
  rowSums(
    sapply(dist.matrix, function(assay){
      a <- rep(0, num.sample)
      a[order(assay[i,])[(num.far.neig+1):num.sample]] <- 1
      a } )
    )
})

table(melt(neigb.farout)$value)

best.pair <- sapply(1:num.sample, function(i) {
  mean.dist <- rowSums(
    sapply(dist.matrix, function(assay){
      assay[i,]
      } )
    )/96
  c(i,order(mean.dist)[2])
})

## if we work in rel intensity value 
dist.matrix <- dlply(core.data, .(assay), function(x) {
  delta.x <- outer(x$rel.dye1, x$rel.dye1, "-")
  delta.y <- outer(x$rel.dye2, x$rel.dye2, "-")
  dist <- sqrt(delta.x^2 + delta.x^2)
  })

#for now; let's keep track of the last 1/3 of the farthest neighbours 
frac.cutoff <- 1/3
num.sample <- 94
num.assay <- 96
num.far.neig <- round(frac.cutoff * 94)


neigb.farout <- sapply(1:num.sample, function(i) {
  rowSums(
    sapply(dist.matrix, function(assay){
      a <- rep(0, num.sample)
      a[order(assay[i,])[(num.far.neig+1):num.sample]] <- 1
      a } )
    )
})

melt.neigb <- melt(neigb.farout)

ggplot(melt(neigb.farout), aes(x=Var1, y=Var2, color=round(num.assay-value, digits=-1)))+geom_point()
table(melt(neigb.farout)$value)
```

   
Of the sturgeon datasets, `1381905043_raw.csv, 1381935339_raw.csv, 1381992037_raw.csv, 1381992302_raw.csv`, there are samples that are genotyped more than once. The questions is whether we are able to determine whether a sample of one run are repeated in another under the circumstance of not knowing the label.   
    
To test it out, I am going to ignore the plate effect. I am going to account for it in later steps to see whether there's any improvement in doing so.   
   
```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
test.FILE.1<-read.csv("1381905043_raw.csv.tbl")
test.FILE.2<-read.csv("1381935339_raw.csv.tbl")
test.FILE.3<-read.csv("1381992037_raw.csv.tbl")
test.FILE.4<-read.csv("1381992302_raw.csv.tbl")

ReOrganizeFile <- function (test.FILE) {
  clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]
  clean.file <- test.FILE[test.FILE$Type=="Unknown",]
  clean.file$Name <- factor(clean.file$Name)
  clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))
  
  core.data <- data.frame(
    assay= as.numeric(clean.file$Assay),
    name = as.numeric(clean.file$Name),
    dye1 = clean.file$chDYE1-clean.file$bgDYE1,
    dye2 = clean.file$chDYE2-clean.file$bgDYE2,
    rel.dye1 = clean.file$Allele.X.1,
    rel.dye2 = clean.file$Allele.Y.1,
    k=as.numeric(clean.file$Final),
    fullname=clean.file$Name)
  
  core.data <- arrange(core.data, assay, name)
  core.data
  }

core.data.1 <- ReOrganizeFile(test.FILE.1)
core.data.2 <- ReOrganizeFile(test.FILE.2)
core.data.3 <- ReOrganizeFile(test.FILE.3)
core.data.4 <- ReOrganizeFile(test.FILE.4)

getCloseness <- function(core.data, sec.core.data, relative=T, frac.cutoff = 2/3) {
  
  dist.matrix <- dlply(core.data, .(assay), function(x) {
    y <- sec.core.data[which(sec.core.data$assay==x$assay[1]),]
    delta.x <- outer(x$rel.dye1, y$rel.dye1, "-")
    delta.y <- outer(x$rel.dye2, y$rel.dye2, "-")
    dist <- sqrt(delta.x^2 + delta.x^2)
    })
  
  num.sample <- length(unique(core.data$name))
  num.sec.sample <- length(unique(sec.core.data$name))
  num.assay <- length(unique(core.data$assay))
  num.far.neig <- round(frac.cutoff * num.sample)
  
  num.loci.neigb <- sapply(1:num.sec.sample, function(i) {
  ct.tbl <- rowSums(
    sapply(dist.matrix, function(assay){
      a <- rep(0, num.sample)
      a[order(assay[,i])[1:num.far.neig]] <- 1
      a } )
    )
  })
  
  max.loci <- apply(num.loci.neigb,
                         2,
                         function (x) sort(x,decreasing=T))
  
  neigb.id <- apply(num.loci.neigb,
                         2,
                         function (x) order(x,decreasing=T))
  
  
  repeat.sample.name <- intersect(core.data$fullname, sec.core.data$fullname)
  
  data1.rp.id <- core.data[match(repeat.sample.name, core.data$fullname),"name"]
  data2.rp.id <- sec.core.data[match(repeat.sample.name, sec.core.data$fullname),"name"]
  
  # preparing ggplot
  melt.neigb <- melt(num.loci.neigb)
  colnames(melt.neigb) <- c("data1", "data2", "loci")

  
  melt.known.same <- cbind("data1" = data1.rp.id,
                           "data2" = data2.rp.id,
                           "loci" = num.loci.neigb[matrix(c(data1.rp.id, data2.rp.id), ncol=2)],
                           "ident" = 1)
  
  melt.neigb<-join(melt.neigb, as.data.frame(melt.known.same))
  melt.neigb
  }

g <- ggplot()
comb.set <- combn(1:4,2)
for ( a in 1:6 ){
  for (i in c(1/3, 1/2, 2/3)){
  data <- getCloseness(get(paste0("core.data.",comb.set[1,a])),
                       get(paste0("core.data.",comb.set[2,a])),
                       frac.cutoff = i)
  data <- as.data.frame(cbind(data,
                              "cut"=i,
                              "Data"=paste("Data ", comb.set[1,a], " and Data ", comb.set[2,a])))
  g <- g + 
    geom_point(data=data, aes(x=data2, y=loci, color=factor(ident)))
  }
}

g<- g+facet_wrap(Data~cut, ncol=3)+
  xlab("Sample ID")+
  ylab("Num of loci within the neigborhood (MAX: 96)")+
  guides(fill=FALSE)
g
#ggsave("NN-sturgeon-set.pdf",g, height=20, width=30)

#View(loci.matrix)

intersect(core.data.1$fullname, core.data.2$fullname)

sec.core.data<-core.data.2[core.data.2$name==56,]
sec.core.data<-core.data[core.data$name<3,]

melt.neigb <- melt(neigb.farout)

ggplot(melt(neigb.farout), aes(x=Var1, y=Var2, color=round(num.assay-value, digits=-1.8)))+geom_point()
table(melt(neigb.farout)$value)


```
   

   
# Supervised learning model

## Data Shaping/Prepping
```{r}
rm(list=ls(all=TRUE))
set.seed(793241)

library(ggplot2)
library(plyr)
library(grid)
library(gridExtra)
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
## This function runs Gibbs sampling based on the regression model
test.FILE=read.csv("1382089116_raw.csv.tbl")

#remove any samples with "NTC", "No Call" and "Invalid" calls ; renew the factor
clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]
clean.file$Name <- factor(clean.file$Name)
clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))
#revalue(factor(clean.file$Final), c("XX"=2, "XY"=1, "YY"=0))

# reshape the test.FILE to get the essential
# (Assay (j), Name (i), Dye1 (y_1), Dye2 (y_2), k (k_1))
core.data <- data.frame(
  assay= as.numeric(clean.file$Assay),
  name = as.numeric(clean.file$Name),
  dye1 = clean.file$chDYE1-clean.file$bgDYE1,
  dye2 = clean.file$chDYE2-clean.file$bgDYE2,
  rel.dye1 = clean.file$Allele.X.1,
  rel.dye2 = clean.file$Allele.Y.1,
  k=as.numeric(clean.file$Final))
```
   
### Creating Training Data set   
   
* Leave one out: 
take 1 locus signal of 1 individual out   
take all locus signal of 1 individual out  
take 1 locus signal of all individuals  

* leave k-pair out:

* cross-validation:

## MODEL 1
   
  
### Hyperparameter Assignment 
     
* via MLE of the data  

This option would be available if we manually curate some of the data beforehand. 

```{r}
# for now, let's train the whole dataset
train.data <- core.data
  
hyper <- NULL
hyper$N <- length(unique(train.data$name))
hyper$M <- length(unique(train.data$assay))
hyper$l <- 2 # for diploid
hyper$mu <- sapply(1:(hyper$l+1), function(x) mean(train.data[train.data$k==x,]$dye1))
hyper$sigma2 <- sapply(1:(hyper$l+1), function(x) var(train.data[train.data$k==x,]$dye1))

dev.mean.tbl <- daply(train.data, "assay", function(x) {
  mean.local <- sapply(1:(hyper$l+1), function(y) mean(x[x$k==y,]$dye1))
  mean.local-hyper$mu
  })

hyper$b0 <- mean(as.vector(dev.mean.tbl), na.rm=T)
hyper$psi0 <- var(as.vector(dev.mean.tbl), na.rm=T)

dev.scale.tbl <- daply(train.data, "assay", function(x) {
  var.local <- sapply(1:(hyper$l+1), function(y) var(x[x$k==y,]$dye1))
  hyper$sigma2/var.local
  })

hyper$b1 <- mean(as.vector(dev.scale.tbl), na.rm=T)
hyper$psi1 <- var(as.vector(dev.scale.tbl), na.rm=T)
  
hyper$a <- 2 # arbitrary; but want to have long spread
hyper$b <- 5
  
# initialize the parameter: empirical bayes style
para <- NULL
para$sigma2 <- hyper$b/(hyper$a-1)  # MLE : mean of inverse gamma

para$b0 <- dev.mean.tbl
para$b1 <- dev.scale.tbl
para$x <- daply(train.data, "name", function (x){
  sapply(1:(hyper$l+1), function(y) mean(x[x$k==y,]$dye1))
  })

#model1.result <- RunMcmcModel1(para=para,
#                               hyper=hyper,
#                               y=train.data,
#                               n.sam=5,
#                               n.burn=0)

```


*  via k-means result
## Regional inference by signal association 

Data: I need to slurp the regional data from `ame_bycatch3916_gsisim.txt`, `ame_bycatch4332_gsisim.txt`, `ame_final_baseline_gsisim.txt`.   
   
```{bash}
2015-04-03 13:51 /preprocess/--% pwd
/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess
echo -e 'Name,Region' > regional.tbl
awk '/AM/{print $1",BC3916"}' ame_bycatch3916_gsisim.txt  >> regional.tbl
awk '/AM/{print $1",BC4332"}' ame_bycatch4332_gsisim.txt >> regional.tbl
awk 'BEGIN{label="North"} {if (/AM/){print $1","label} if(/SOUTH/){label="South"}}' ame_final_baseline_gsisim.txt >> regional.tbl
```

Let's work with the Sturgeon dataset

```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
test.FILE=read.csv("1381905043_raw.csv.tbl")
test.FILE1=read.csv("1381992037_raw.csv.tbl")
test.FILE <- rbind(test.FILE, test.FILE1)


region.FILE=read.csv("regional.tbl")
test.FILE <- join(test.FILE, region.FILE)

clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]

clean.file <- test.FILE[test.FILE$Type=="Unknown",]
clean.file$Name <- factor(clean.file$Name)
clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))
#revalue(factor(clean.file$Final), c("XX"=2, "XY"=1, "YY"=0))

# reshape the test.FILE to get the essential
# (Assay (j), Name (i), Dye1 (y_1), Dye2 (y_2), k (k_1))
core.data <- data.frame(
  assay= as.numeric(clean.file$Assay),
  name = as.numeric(clean.file$Name),
  dye1 = clean.file$chDYE1-clean.file$bgDYE1,
  dye2 = clean.file$chDYE2-clean.file$bgDYE2,
  rel.dye1 = clean.file$Allele.X.1,
  rel.dye2 = clean.file$Allele.Y.1,
  k=as.numeric(clean.file$Final),
  region=clean.file$Region)


#core.data <- arrange(core.data, assay, name)

## the next step is to create a weighted mean distance matrix, where the weight is determine by the kernel density of the genotype signal. If the signal is spread widely, the weighted for the genotype panel is higher. If it is concentrated, then it will reduced.

assay.density <- daply(core.data, .(assay), function(x) {
  cut.size<-300
  ## absolute value
  #density <- kde2d(x=x$dye1, y=x$dye2,n=cut.size, lims=c(c(0,20000),c(0,20000)))
  ## relative intensity value
  density <- kde2d(x=x$rel.dye1, y=x$rel.dye2,n=cut.size, lims=c(c(0,2),c(0,2)))
  #sum(density$z)
  #max(density$z)
  sum(density$z>1e-10)/(cut.size^2)
  })

norm.weight.assay <- assay.density/sum(assay.density)


## consider the absolute intensity value
weighted.dist.matrix <- dlply(core.data, .(assay), function(x) {
  #delta.x <- outer(x$dye1, x$dye1, "-")
  #delta.y <- outer(x$dye2, x$dye2, "-")
  # for relative inten
  delta.x <- outer(x$rel.dye1, x$rel.dye1, "-")
  delta.y <- outer(x$rel.dye2, x$rel.dye2, "-")
  
  # weighted or unweighted option
  sqrt(delta.x^2 + delta.x^2) #*norm.weight.assay[x$assay]
  })

#for now; let's keep track of the last 1/3 of the farthest neighbours 
#frac.cutoff <- 1/3
num.sample <- 188#length(unique(core.data$name))
num.assay <- 96
#num.far.neig <- round(frac.cutoff * 94)

# keeps track of the farthest neighbours
mean.dist <- sapply(1:num.sample, function(i) {
  rowSums(
    sapply(weighted.dist.matrix, function(assay){
      assay[i,]
      } )
    )/num.assay
})

melt.mean <- melt(mean.dist)
colnames(melt.mean) <- c("name", "indiv", "mean_dist")
melt.mean <- join(melt.mean, unique(core.data[c("name","region")]) )

g<- ggplot(melt.mean, aes(y=factor(region), x=mean_dist, color=factor(region)))+
  geom_point()+scale_x_log10()+
  facet_wrap(~indiv, ncol=10)
#ggsave("mean-dist-sturgeonset.pdf",g, height=20, width=30)

# I would like to know for those "known-regional" sample, does the nearest neighbour provide some evidence to confirm its regional identity

#isolate the nothern and southern sample
name.region <- arrange(unique(core.data[c("name","region")]), name)
ns.ind <- c(which(name.region$region == "North"), which(name.region$region == "South"))
ns.region <- name.region[ns.ind,]

mean.ds.ns <- mean.dist[ns.ind,ns.ind]
row.names(ns.region)<-1:58
is.match.next <- apply(mean.ds.ns,1, function(x) {
  order.ind <- order(x)
  region1 <- ns.region[order.ind[1],2]
  region2 <- ns.region[order.ind[2],2]
  region1 ==region2
  })
sum(is.match.next)
```
Based on the combined nonadjusted relative intensity value, 54 out of 58 (93.1%) "known-regional" individuals matches with the regional label of the nearest neighbor.    

What i have discovered from this:
   
 * using a relative intensity value gives a better regional predictive result 
 * whether mean is weight or not does not make much of a difference.

# Source 

## Preprocessing script 

function: reformat the raw intensity csv excel file into R-friendly csv table  
  
```{bash}
#/Users/eric.crandall/Desktop/tng/fluidigm/data/raw
for i in *_raw.csv ; do
perl -e 'chomp;
${file_prefix}=$ARGV[0]; 
open FILE, ${file_prefix}; 
open O, ">".${file_prefix}.".".++$n; 
while(<FILE>){
if(/^\s*$/) { close O; open O, ">".${file_prefix}.".".++$n; }
else { print O $_;}
}' $i;
awk 'NR>2' ${i}.3 |perl -p -e 's/\r//g' > ../preprocess/${i}.exp;
paste -d "," ${i}.5 ${i}.6 ${i}.7 ${i}.8 ${i}.9 ${i}.10 |awk 'BEGIN{FS=","}{ if(NR>2){print $1","$2","$5","$8","$11","$14","$17}}' > ../preprocess/${i}.raw;

cat ../preprocess/${i}.exp ../preprocess/${i}.raw| awk 'BEGIN{FS=","} {if(NR==1){print $0 ",chDOX, chDYE1, chDYE2, bgDOX, bgDYE1, bgDYE2"} else{e[$1]++; if(e[$1]==2){print d[$1]","$2","$3","$4","$5","$6","$7} d[$1]=$0;  }}' > ../preprocess/${i}.tbl;
rm ../preprocess/${i}.exp ../preprocess/${i}.raw;
done
rm *_raw.csv.*
```


## Gibbs model 1
```{r, eval=FALSE}  
## This function runs Gibbs sampling based on the regression model
RunMcmcModel1 <- function(para, hyper, y, n.sam, n.burn)
{
  ###cache the draw
  save.sam <- NULL
  save.sam$b0 <- lapply(1:n.sam, function(i) array(NA, dim=c(hyper$M, hyper$l+1)))
  save.sam$b1 <- lapply(1:n.sam, function(i) array(NA, dim=c(hyper$M, hyper$l+1)))
  save.sam$x <- lapply(1:n.sam, function(i) array(NA, dim=c(hyper$N, hyper$l+1)))
  save.sam$sigma2 <- rep(NA, n.sam)
  
  ##call updating for N.sam iterations
  for(i in 1:n.sam)
  {
    
    if(i%%5==1) {
      print(c("Complete: ", i))
    }
    
    para$b0 <- UpdateB0(para, hyper, y)
    para$b1 <- UpdateB1(para, hyper, y)
    para$x <- UpdateX(para, hyper, y)
    para$sigma2 <- UpdateSigma2(para, hyper, y)
    
    save.sam$b0[[i]] <- para$b0
    save.sam$b1[[i]] <- para$b1
    save.sam$x[[i]] <- para$x
    save.sam$sigma2[i] <- para$sigma2
  }
  ### clean out burn-in
  
  save.sam$b0 <- save.sam$b0[-(1:n.burn)]
  save.sam$b1 <- save.sam$b1[-(1:n.burn)]
  save.sam$x <- save.sam$x[-(1:n.burn)]
  save.sam$sigma2 <- save.sam$sigma2[-(1:n.burn)]
  return(save.sam)
}

##Hyperparameter: hyper$M, hyper$l [=2], hyper$psi0, hyper$psi1, hyper$N, hyper$mu[k]

#updating parameter B0
UpdateB0<- function(para, hyper, y){
  b0.val <- array(NA, dim=c(hyper$M, hyper$l+1))
  for (j in 1:hyper$M) {
    for (k in 1:(hyper$l+1)) {
      iso.data <- y[y$assay==j & y$k==k,]
      x.val <- para$x[iso.data$name,k]
      y.val <- iso.data$dye1
      n.i <- length(x.val)
      
      normal.var <- (n.i/para$sigma2 + 1/hyper$psi0)^(-1)
      normal.mean <- normal.var * ( ( sum(y.val)-para$b1[j,k]*sum(x.val) ) / para$sigma2 + hyper$b0 / hyper$psi0)
      b0.val[j,k] <- rnorm(1, normal.mean, sqrt(normal.var))
    }
  }
  return(b0.val)
}

UpdateB1<- function(para, hyper, y){
  b1.val <- array(NA, dim=c(hyper$M, hyper$l+1))
  for (j in 1:hyper$M) {
    for (k in 1:(hyper$l+1)) {
      iso.data <- y[y$assay==j & y$k==k,]
      x.val <- para$x[iso.data$name,k]
      y.val <- iso.data$dye1
      n.i <- length(x.val)
      
      normal.var <- (sum(x.val)/para$sigma2 + 1/hyper$psi1)^(-1)
      normal.mean <- normal.var * ( ( sum(y.val*x.val)-para$b0[j,k]*sum(x.val) ) / para$sigma2 + hyper$b1 / hyper$psi1)
      b1.val[j,k] <- rnorm(1, normal.mean, sqrt(normal.var))
      #print(b1.val[j,k])
  }
  }
  return(b1.val)
}

UpdateX <- function(para, hyper, y) {
  x.val <- array(NA, dim=c(hyper$N, hyper$l+1))
  for (i in 1:hyper$N) {
    for (k in 1:(hyper$l+1)) {
      iso.data <- y[y$name==i & y$k==k,]
      
      b1.val <- para$b0.val[iso.data$assay,k]
      b0.val <- para$b1.val[iso.data$assay,k]
      
      y.val <- iso.data$dye1
      n.i <- length(y.val)
      
      normal.var <- (sum(b1.val)/para$sigma2 + 1/hyper$sigma2[k])^(-1)
      normal.mean <- normal.var * ( ( sum(y.val*b1.val)-sum(b0.val*b1.val) ) / para$sigma2 + hyper$mu[k] / hyper$sigma2[k])
      x.val[i,k] <- rnorm(1, normal.mean, sqrt(normal.var))
    }
  }
  return(x.val)
}

UpdateSigma2<- function(para, hyper, y){
  
  select.jk <- cbind(y$assay,y$k)
  select.ik <- cbind(y$name,y$k)
    
  ig.alpha <- hyper$a + (0.5)*length(y$name)
  ig.beta <- hyper$b + (0.5)* sum(
    (y$dye1- (para$b0[select.jk] + para$b1[select.jk]*para$x[select.ik])^2))
  
  sigma2.val <- 1/rgamma(1, shape=ig.alpha, rate=ig.beta)
  return(sigma2.val)
}
```
