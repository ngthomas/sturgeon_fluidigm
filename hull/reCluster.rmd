---
title: "Plate to Plate Correction"
author: "Thomas Ng"
date: "April 16, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
library(ggplot2)
library(plyr)
library(grid)
library(gridExtra)
library(dplyr)
library(reshape2)
library(MASS)
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
```



http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030906
http://edepot.wur.nl/177104

Goal: To reduce plate-to-plate effect by readjusting all of the relative intensity based on signal deviance of a common samples

```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
test.FILE.1<-read.csv("1381905043_raw.csv.tbl")
test.FILE.2<-read.csv("1381935339_raw.csv.tbl")
test.FILE.3<-read.csv("1381992037_raw.csv.tbl")
test.FILE.4<-read.csv("1381992302_raw.csv.tbl")


sturgeon.FILE <- rbind(cbind("plate"=1, test.FILE.1),
                       cbind("plate"=2, test.FILE.2),
                       cbind("plate"=3, test.FILE.3),
                       cbind("plate"=4, test.FILE.4))


ReOrganizeFile <- function (test.FILE) {
  #clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]
  clean.file <- test.FILE[test.FILE$Type=="Unknown",]
  #clean.file <- test.FILE
  clean.file$Name <- factor(clean.file$Name)
  clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))#,"NTC"))
  clean.file$Assay <- factor(clean.file$Assay, levels = unique(clean.file$Assay))
  
  core.data <- data.frame(
    plate = clean.file$plate,
    assay= as.numeric(clean.file$Assay),
    name = as.numeric(clean.file$Name),
    dye1 = clean.file$chDYE1-clean.file$bgDYE1,
    dye2 = clean.file$chDYE2-clean.file$bgDYE2,
    rel.dye1 = clean.file$Allele.X.1,
    rel.dye2 = clean.file$Allele.Y.1,
    k=as.numeric(clean.file$Final),
    full.name=clean.file$Name,
    assay.name=clean.file$Assay
    )
  
  core.data <- arrange(core.data, assay, name)
  core.data
  }

core.data <- ReOrganizeFile(sturgeon.FILE)
#g <- ggplot(core.data, aes(x=rel.dye1, y=rel.dye2, color=factor(plate))) +
#  geom_point()+
#  facet_wrap(~assayname, ncol = 10 )

core.pdata<-tbl_df(core.data)

repeat.data <- core.pdata %>%
  group_by(assay,name) %>%
  summarise(count = n()) %>%
  filter(count > 1) %>%
  select(assay, name) %>%
  inner_join(core.pdata) 

#demf <- function(x) {dim(x)[1]}
#sam.data %>%
#  group_by(assay,name) %>%
#  mutate_each(newcol=funs(demf), assay) function(x) {
#    dim(x)[0]
#  })


list.seg<-ddply(repeat.data, .(assay, name), function(x) {
  num.rep <- dim(x)[1]
  all.combn <- combn(1:num.rep, 2)  
  seg.pt <- t(apply(all.combn, 2, function(i) {
    c(x$rel.dye1[i[1]], 
      x$rel.dye2[i[1]],
      x$rel.dye1[i[2]],
      x$rel.dye2[i[2]],
      paste(sort(c(x$plate[i[1]], 
                 x$plate[i[2]])), collapse="_"),
      as.character(x$assay.name[1])
                 )}))
  
  #seg.pt <- as.data.frame(seg.pt)
  colnames(seg.pt)<- c("x.start", "y.start", "x.end", "y.end", "plate.pair", "assay.name")
  seg.pt
})

## I need to find a better wait to do this; it is a bit annoying of how this numeric turns into levels
list.seg$x.start <- as.numeric(as.character(list.seg$x.start))
list.seg$y.start <- as.numeric(as.character(list.seg$y.start))
list.seg$x.end <- as.numeric(as.character(list.seg$x.end))
list.seg$y.end <- as.numeric(as.character(list.seg$y.end))

list.seg$assay.name <- factor(list.seg$assay.name,levels(core.data$assay.name))

#%>% filter(as.integer(assay.name)<=48) %>% droplevels()

n.groups <- 4
n.loci <- 96

for (i in 1:(n.groups)) {
  
  min.intv <- round(n.loci/n.groups)*(i-1)
  max.intv <- min.intv + round(n.loci/n.groups)
  g <- ggplot(data=core.data %>% 
                filter(as.integer(assay.name)>min.intv, as.integer(assay.name) <= max.intv) %>% 
                droplevels(), 
              aes(x=rel.dye1, y=rel.dye2, color=factor(plate))) +
    geom_point(alpha=0.7)+
    geom_segment(data=list.seg %>% 
                   filter(as.integer(assay.name)>min.intv, as.integer(assay.name) <= max.intv) %>%
                   droplevels(),
                 aes(x=x.start, y=y.start, xend=x.end, yend=y.end, color=plate.pair), linetype=5)+
    facet_wrap(~assay.name, ncol = 6 )+
    theme_bw()
  
  ggsave(paste0("four-plates-xy_",i,".pdf"),g, width = 34, height = 22)
}

## for raw graph

for (i in 1:(n.groups)) {
  
  min.intv <- round(n.loci/n.groups)*(i-1)
  max.intv <- min.intv + round(n.loci/n.groups)
  g <- ggplot(data=core.data %>% 
                filter(as.integer(assay.name)>min.intv, as.integer(assay.name) <= max.intv) %>% 
                droplevels(), 
              aes(x=dye1, y=dye2, color=factor(plate))) +
    geom_point(alpha=0.7)+
    geom_segment(data=list.seg %>% 
                   filter(as.integer(assay.name)>min.intv, as.integer(assay.name) <= max.intv) %>%
                   droplevels(),
                 aes(x=x.start, y=y.start, xend=x.end, yend=y.end, color=plate.pair), linetype=5)+
    facet_wrap(~assay.name, ncol = 6 )+
    theme_bw()
  
  ggsave(paste0("raw_four-plates-xy_",i,".pdf"),g, width = 34, height = 22)
}


skew.summary <- list.seg %>%
  mutate(distance=sqrt((x.start-x.end)^2 +(y.start-y.end)^2),
         slope = (y.start-y.end)/(x.start-x.end)) 

ggplot(skew.summary, aes(x=distance, y=slope, color=plate.pair))+
  geom_point()

```


After printing out the combined runs, I hand annotated all of the cluster, and assign call for each separate run using the fluidigm sofware. I have to call each run individually since the 'combined run options' renormalize all of relative signal intensity.   
  
All of the assigned csv files can be found in `/Users/eric.crandall/Desktop/tng/fluidigm/data/raw/recall`.
   
I need to process it: 

```{bash}
cd /Users/eric.crandall/Desktop/tng/fluidigm/data/raw/recall
for i in *.csv ; do
perl -e 'chomp;
${file_prefix}=$ARGV[0]; 
open FILE, ${file_prefix}; 
open O, ">".${file_prefix}.".".++$n; 
while(<FILE>){
if(/^\s*$/) { close O; open O, ">".${file_prefix}.".".++$n; }
else { print O $_;}
}' $i;
awk 'NR>2' ${i}.3 |perl -p -e 's/\r//g' > ../../preprocess/recall1_${i};
done
```

# Data Exploratory post manual annotation

```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
recall.FILE.1<-read.csv("recall1_1381905043.csv")
recall.FILE.2<-read.csv("recall1_1381935339.csv")
recall.FILE.3<-read.csv("recall1_1381992037.csv")
recall.FILE.4<-read.csv("recall1_1381992302.csv")


combn.recall.FILE <- rbind(cbind("plate"=1, recall.FILE.1),
                       cbind("plate"=2, recall.FILE.2),
                       cbind("plate"=3, recall.FILE.3),
                       cbind("plate"=4, recall.FILE.4))

recall.df <- ReOrganizeFile(combn.recall.FILE)

# reassign K - the number of cluster due to limitations of fluidigm' scoring capability
recall.k.df <- recall.df %>%
  group_by(assay) %>%
  mutate(new.k = SpanningK(k, rel.dye1, rel.dye2)) %>%
  mutate(total.k = max(new.k))


# introducing the regional labels
region.FILE=read.csv("regional.tbl")
colnames(region.FILE) <- c("full.name", "region")
make_region_unk <- function(region) {ifelse(is.na(region),"unk",region)}

# merging the scoring file with the regional files
recall.k.region <- left_join(recall.k.df, region.FILE) %>%
  mutate(region = make_region_unk(as.character(region)))

# Since there are duplicated samples across four plates, I need to 
# reduce any duplicated samples and pick the one that with k>0 and the highest sum rel-xy signal
reduce.data <- recall.k.region %>% 
  group_by(assay, name) %>% 
  arrange(.,desc(k>0), desc(rel.dye1+rel.dye2)) %>% 
  filter(row_number()==1)

# reformating the "reduce.data"set into known-regional training dataset (North and South)

north.obs <- reduce.data %>%
  filter(region %in% "North") %>%
  group_by(assay.name) %>%
  summarise(ct.0=sum(new.k==0),
            ct.1=sum(new.k==1),
            ct.2=sum(new.k==2),
            ct.3=sum(new.k==3),
            ct.4=sum(new.k==4),
            ct.5=sum(new.k==5),
            max.k =max(total.k) )

south.obs <- reduce.data %>%
  filter(region %in% "South") %>%
  group_by(assay.name) %>%
  summarise(ct.0=sum(new.k==0),
            ct.1=sum(new.k==1),
            ct.2=sum(new.k==2),
            ct.3=sum(new.k==3),
            ct.4=sum(new.k==4),
            ct.5=sum(new.k==5),
            max.k =max(total.k))

#ggplot(reduce.data, aes(x=rel.dye1, y=rel.dye2, color=factor(new.k>0)))+
#  geom_density2d()

```


```{r}

# Leave-one-out
# Take one of the "North" sample out and re-calculate its predictive label for the "North" training dataset
reduce.data %>%
  filter(region %in% "North",total.k > 0) %>%
  arrange(name, assay) %>%
  group_by(name) %>%
  summarise(prob.north =get.regional.prob(assay, new.k, region, for.region="North", take.one.out=T), 
            prob.south =get.regional.prob(assay, new.k, region, for.region="South", take.one.out=T) ) %>%
  mutate(prob.being.north=prob.north/(prob.north+prob.south))

# LOU
#Take one South sample out and re-calculate its predictive label
reduce.data %>%
  filter(region %in% "South",total.k > 0) %>%
  arrange(name, assay) %>%
  group_by(name) %>%
  summarise(prob.north =get.regional.prob(assay, new.k, region, for.region="North", take.one.out=T), 
            prob.south =get.regional.prob(assay, new.k, region, for.region="South", take.one.out=T) ) %>%
  mutate(prob.being.north=prob.north/(prob.north+prob.south))

# Calculate the predictive probabilities for by-catch label
region.unk.result <- reduce.data %>%
  filter(!(region %in% c("North","South")),total.k > 0) %>%
  arrange(name, assay) %>%
  group_by(name) %>%
  summarise(prob.north =get.regional.prob(assay, new.k, region, for.region="North"), 
            prob.south =get.regional.prob(assay, new.k, region, for.region="South"),
            region.lab = region[1]) %>%
  mutate(prob.being.north=prob.north/(prob.north+prob.south))

ggplot(region.unk.result, aes(x=name, y=prob, color=region.lab))+
  geom_point()
```

```{r}

# tallying the number of uncalled loci per individual; the higher the number, the higher the chances for that individual being a poor sample
reduce.data %>%
  filter(total.k > 0) %>%
  arrange(name) %>% group_by(name) %>% filter(new.k==0) %>%
  tally() %>%
  select(n) %>% 
  table


ggplot(reduce.data %>% filter(total.k > 0, new.k >0, region %in% c("North","South")), aes(x=assay, y=full.name, z=new.k, color=factor(new.k)))+
  facet_wrap(~region, ncol=1, drop=T, scale="free_y")+
  geom_point()+
  theme_bw()

# get chi-seq p value / or K-S score in the future
p.val.NS <- apply(cbind(north.obs, south.obs) , 1 , function(x) {
    num.k <- as.numeric(x[16])
    chisq.test(cbind(1+as.numeric(x[3:(3+num.k-1)]),
                    1+as.numeric(x[11:(11+num.k-1)])))$p.val
    })



### the whole point of the below is to make the graph that inform which markers are helpful in deciding the region that individuals are most likely belonged to 

NS.table <- as.data.frame(rbind(cbind(north.obs, 
                                      region="North", 
                                      p.val = p.val.NS),
                                cbind(south.obs, 
                                      region="South", 
                                      p.val = p.val.NS)))
melt.table <- melt(NS.table,id.vars = c("assay.name", "region", "p.val")) %>% 
  filter(value>0, !(variable %in% c("ct.0", "max.k"))) 

melt.table$assay.name <- with(melt.table, reorder(assay.name, -p.val))

melt.table.frac.N <- melt.table %>% 
  filter(region=="North") %>% 
  mutate(fraction=value/21)

melt.table.frac.S <- melt.table %>% 
  filter(region=="South") %>% 
  mutate(fraction=value/39)

ggplot(rbind(melt.table.frac.N, melt.table.frac.S), aes(x=assay.name,y=fraction, fill=variable))+
  facet_wrap(~region)+
  geom_bar(stat = "identity",position = "stack")+
  coord_flip()+
  scale_fill_brewer(palette = "Set2", name="cluster #", labels=c(1,2,3,4,5))+
  ylab("Group fraction")+
  xlab("Locus Name - sort by Chi-seq value")
```




## PCA plot
The PCA analysis is not accurate because the data contains the uncalled "0" categories.  
  
```{r}

df.for.pca <-reduce.data %>% filter(total.k > 0) 

total.assay <- max(df.for.pca$assay)
total.name <- max(df.for.pca$name)

# initialization for the PCA data matrix
data.for.pca <- matrix(0, ncol=total.assay, nrow=total.name)
data.for.pca[cbind(df.for.pca$name, df.for.pca$assay)] <- df.for.pca$new.k

# remove all rows that are zeros; another word removing the un-clusterable loci
nonzero.row <- apply(data.for.pca, 1, sum) > 0
pca.raw <- data.for.pca[nonzero.row,]

pca <- prcomp(pca.raw, scale=F)

chisq.rval <- round(log(p.val.NS))
pca.melted <- melt(pca$rotation[,1:9])
pca.label <- cbind(pca.melted, chisq.rval)

pca.label$Var1 <- with(pca.label, reorder(Var1, -chisq.rval))

ggplot(data=pca.label) +
  geom_bar(aes(x=Var1, y=value, fill=factor(chisq.rval)), stat="identity") +
  facet_wrap(~Var2)

sample.groups <- reduce.data %>% group_by(name) %>% summarise(regional.labels=region[1],
                                                              uniq.name =name[1]) %>% .[,2:3]
scores <- tbl_df(data.frame(sample.groups, pca$x[,1:3]))

ggplot(scores %>% filter(regional.labels %in% c("North","South")), 
       aes(x=PC1, y=PC2, colour=regional.labels))+
  geom_point(size=4, alpha=0.8)+
  geom_point(data=scores %>% filter(!regional.labels %in% c("North","South")),
             aes(x=PC1, y=PC2, colour=regional.labels),
             size=2)+
  scale_color_discrete("Regional Labels", breaks=c("North", "South", "BC3916", "BC4332", "unk"))+
  theme_bw()


```


# Testing out identifiability of duplicated samples between plates

```{r}
unmelt.rpnk <- dcast(recall.k.region %>% filter(total.k>0), region+plate+name~assay, value.var="new.k")
n.sample <- nrow(unmelt.rpnk)


comb.set <- cbind(comb.set,rbind(1:4,1:4))
plot.by.ident <- vector("list",ncol(comb.set))
plot.by.region <- vector("list",ncol(comb.set))

for ( set in 1:ncol(comb.set) ){
  
  set.1 <-  unmelt.rpnk %>% filter(plate==comb.set[1,set]) 
  set.2 <-  unmelt.rpnk %>% filter(plate==comb.set[2,set]) 
  
  k.set.1 <- set.1 %>% .[,4:77]
  k.set.2 <- set.2 %>% .[,4:77]
  
  name.set.1 <- set.1 %>% .[,3]
  name.set.2 <- set.2 %>% .[,3]
  
  region.set.1 <- set.1 %>% .[,1]
  region.set.2 <- set.2 %>% .[,1]
  
  same.k <- apply(k.set.1,1,function(x) colSums(x==t(k.set.2)))
  same.0 <- apply(k.set.1,1,function(x) colSums(x+t(k.set.2)==0))
  melt.sharek <- melt(same.k-same.0)
  
  g <- ggplot(melt.sharek, aes(x=Var1, y=value))+
    geom_point(alpha=0.2)+
    xlab(paste("Sample ID from plate ", comb.set[2,set],collapse=""))+
    ylab(paste("Number of loci shared with plate ",comb.set[1,set], collapse="") )

  inters.name <- intersect(name.set.1, name.set.2)
  
  if(length(inters.name)>0){
    ind.inters.set1 <- which(name.set.1 %in% inters.name)
    ind.inters.set2 <- which(name.set.2 %in% inters.name)
  
    val <- (same.k-same.0)[cbind(ind.inters.set2, ind.inters.set1)]
    inters.df <- data.frame(x=ind.inters.set2, y=val)
    
    g <- g + 
      geom_point(data=inters.df, aes(x=x, y=y, color="blue"))+
      scale_color_discrete("", "identical Sample")
  }
  plot.by.ident[[set]]  <- g 
  
  
  melt.share.k.r <- melt.sharek %>% 
    mutate(reg.1=region.set.2[Var1], reg.2 =region.set.1[Var2])
  
  melt.NS <- melt.share.k.r %>% 
    filter(reg.1 %in% c("North", "South"), reg.2 %in% c("North", "South")) %>% 
    mutate(compare.reg = paste(reg.1,reg.2,sep="-") )#ifelse(reg.1!=reg.2, paste(reg.1,reg.2,sep="-"),reg.1)) 
  
  g.reg <- ggplot(melt.sharek, aes(x=Var1, y=value, color))+
    geom_point(alpha=0.2)+
    xlab(paste("Sample ID from plate ", comb.set[2,set],collapse=""))+
    ylab(paste("Number of loci shared with plate ",comb.set[1,set], collapse="") )
  
  if(ncol(melt.NS)>0){
    g.reg <- g.reg + 
      geom_point(data=melt.NS, aes(x=Var1, y=value, color=compare.reg))
  }
  plot.by.region[[set]]  <- g.reg 
    
}

pdf("num-of-shared-k-per-locus-by-region.pdf", height=20, width=30)
multiplot(plotlist=plot.by.region, cols=4)
dev.off()

pdf("num-of-shared-k-per-locus-by-id.pdf", height=20, width=30)
multiplot(plotlist=plot.by.ident, cols=4)
dev.off()

g <- multiplot(plotlist=plot.by.ident, cols=3)
ggsave("num-of-shared-k-per-locus-by-id.pdf",g, height=20, width=30)


#same.k.matrix <- sapply(1, function(i) {
#  t(sapply(1:n.sample, function(j) {
#    n.share <- sum(unmelt.k[i,]==unmelt.k[j,])
    #n.0 <- sum((unmelt.k[i,]+unmelt.k[j,])==0)
    #n.share - n.0
#  }))
#})

```


Might need to enforce a minimal signal 






# Supervised learning 
  
 * posterior cluster assginment based on NN-cluster
 * based on cluster-centroid
 * based on smooth unspecificed kernel density
 * based on MVN 



- module for make

```
library(scales)
reverselog_trans <- function(base = exp(1)) {
    trans <- function(x) -log(x, base)
    inv <- function(x) base^(-x)
    trans_new(paste0("reverselog-", format(base)), trans, inv, 
              log_breaks(base = base), 
              domain = c(1e-100, Inf))
}
```


- define multiplot
```{r, eval=FALSE}
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
```