---
title: "Plate to Plate Correction"
author: "Thomas Ng"
date: "April 16, 2015"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
library(ggplot2)
library(plyr)
library(grid)
library(gridExtra)
library(dplyr)
library(reshape2)
library(MASS)
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
```



http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030906
http://edepot.wur.nl/177104

Goal: To reduce plate-to-plate effect by readjusting all of the relative intensity based on signal deviance of a common samples

```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
test.FILE.1<-read.csv("1381905043_raw.csv.tbl")
test.FILE.2<-read.csv("1381935339_raw.csv.tbl")
test.FILE.3<-read.csv("1381992037_raw.csv.tbl")
test.FILE.4<-read.csv("1381992302_raw.csv.tbl")


sturgeon.FILE <- rbind(cbind("plate"=1, test.FILE.1),
                       cbind("plate"=2, test.FILE.2),
                       cbind("plate"=3, test.FILE.3),
                       cbind("plate"=4, test.FILE.4))


ReOrganizeFile <- function (test.FILE) {
  #clean.file <- test.FILE[grep("[X|Y]", test.FILE$Final),]
  clean.file <- test.FILE[test.FILE$Type=="Unknown",]
  #clean.file <- test.FILE
  clean.file$Name <- factor(clean.file$Name)
  clean.file$Final <-factor(clean.file$Final, levels=c("YY","XY", "XX"))#,"NTC"))
  clean.file$Assay <- factor(clean.file$Assay, levels = unique(clean.file$Assay))
  
  core.data <- data.frame(
    plate = clean.file$plate,
    assay= as.numeric(clean.file$Assay),
    name = as.numeric(clean.file$Name),
    dye1 = clean.file$chDYE1-clean.file$bgDYE1,
    dye2 = clean.file$chDYE2-clean.file$bgDYE2,
    rel.dye1 = clean.file$Allele.X.1,
    rel.dye2 = clean.file$Allele.Y.1,
    k=as.numeric(clean.file$Final),
    full.name=clean.file$Name,
    assay.name=clean.file$Assay
    )
  
  core.data <- arrange(core.data, assay, name)
  core.data
  }

core.data <- ReOrganizeFile(sturgeon.FILE)
#g <- ggplot(core.data, aes(x=rel.dye1, y=rel.dye2, color=factor(plate))) +
#  geom_point()+
#  facet_wrap(~assayname, ncol = 10 )

core.pdata<-tbl_df(core.data)

repeat.data <- core.pdata %>%
  group_by(assay,name) %>%
  summarise(count = n()) %>%
  filter(count > 1) %>%
  select(assay, name) %>%
  inner_join(core.pdata) 

#demf <- function(x) {dim(x)[1]}
#sam.data %>%
#  group_by(assay,name) %>%
#  mutate_each(newcol=funs(demf), assay) function(x) {
#    dim(x)[0]
#  })


list.seg<-ddply(repeat.data, .(assay, name), function(x) {
  num.rep <- dim(x)[1]
  all.combn <- combn(1:num.rep, 2)  
  seg.pt <- t(apply(all.combn, 2, function(i) {
    c(x$rel.dye1[i[1]], 
      x$rel.dye2[i[1]],
      x$rel.dye1[i[2]],
      x$rel.dye2[i[2]],
      paste(sort(c(x$plate[i[1]], 
                 x$plate[i[2]])), collapse="_"),
      as.character(x$assay.name[1])
                 )}))
  
  #seg.pt <- as.data.frame(seg.pt)
  colnames(seg.pt)<- c("x.start", "y.start", "x.end", "y.end", "plate.pair", "assay.name")
  seg.pt
})

## I need to find a better wait to do this; it is a bit annoying of how this numeric turns into levels
list.seg$x.start <- as.numeric(as.character(list.seg$x.start))
list.seg$y.start <- as.numeric(as.character(list.seg$y.start))
list.seg$x.end <- as.numeric(as.character(list.seg$x.end))
list.seg$y.end <- as.numeric(as.character(list.seg$y.end))

list.seg$assay.name <- factor(list.seg$assay.name,levels(core.data$assay.name))

#%>% filter(as.integer(assay.name)<=48) %>% droplevels()

n.groups <- 4
n.loci <- 96

for (i in 1:(n.groups)) {
  
  min.intv <- round(n.loci/n.groups)*(i-1)
  max.intv <- min.intv + round(n.loci/n.groups)
  g <- ggplot(data=core.data %>% 
                filter(as.integer(assay.name)>min.intv, as.integer(assay.name) <= max.intv) %>% 
                droplevels(), 
              aes(x=rel.dye1, y=rel.dye2, color=factor(plate))) +
    geom_point(alpha=0.7)+
    geom_segment(data=list.seg %>% 
                   filter(as.integer(assay.name)>min.intv, as.integer(assay.name) <= max.intv) %>%
                   droplevels(),
                 aes(x=x.start, y=y.start, xend=x.end, yend=y.end, color=plate.pair), linetype=5)+
    facet_wrap(~assay.name, ncol = 6 )+
    theme_bw()
  
  ggsave(paste0("four-plates-xy_",i,".pdf"),g, width = 34, height = 22)
}




skew.summary <- list.seg %>%
  mutate(distance=sqrt((x.start-x.end)^2 +(y.start-y.end)^2),
         slope = (y.start-y.end)/(x.start-x.end)) 

ggplot(skew.summary, aes(x=distance, y=slope, color=plate.pair))+
  geom_point()

```


After printing out the combined runs, I hand annotated all of the cluster, and assign call for each separate run using the fluidigm sofware. I have to call each run individually since the 'combined run options' renormalize all of relative signal intensity.   
  
All of the assigned csv files can be found in `/Users/eric.crandall/Desktop/tng/fluidigm/data/raw/recall`.
   
I need to process it: 

```{bash}
cd /Users/eric.crandall/Desktop/tng/fluidigm/data/raw/recall
for i in *.csv ; do
perl -e 'chomp;
${file_prefix}=$ARGV[0]; 
open FILE, ${file_prefix}; 
open O, ">".${file_prefix}.".".++$n; 
while(<FILE>){
if(/^\s*$/) { close O; open O, ">".${file_prefix}.".".++$n; }
else { print O $_;}
}' $i;
awk 'NR>2' ${i}.3 |perl -p -e 's/\r//g' > ../../preprocess/recall1_${i};
done
```

# Data Exploratory post manual annotation

```{r}
setwd("/Users/eric.crandall/Desktop/tng/fluidigm/data/preprocess")
recall.FILE.1<-read.csv("recall1_1381905043.csv")
recall.FILE.2<-read.csv("recall1_1381935339.csv")
recall.FILE.3<-read.csv("recall1_1381992037.csv")
recall.FILE.4<-read.csv("recall1_1381992302.csv")


combn.recall.FILE <- rbind(cbind("plate"=1, recall.FILE.1),
                       cbind("plate"=2, recall.FILE.2),
                       cbind("plate"=3, recall.FILE.3),
                       cbind("plate"=4, recall.FILE.4))


ReOrganizeFile <- function (test.FILE) {
  clean.file <- test.FILE[test.FILE$Type=="Unknown",]
  clean.file$Name <- factor(clean.file$Name)
  clean.file$Final <-factor(clean.file$Final, levels=c("No Call","XX","XY", "YY", "Invalid"))
  clean.file$Assay <- factor(clean.file$Assay, levels = unique(clean.file$Assay))
  
  core.data <- data.frame(
    plate = clean.file$plate,
    assay= as.numeric(clean.file$Assay),
    name = as.numeric(clean.file$Name),
    rel.dye1 = clean.file$Allele.X.1,
    rel.dye2 = clean.file$Allele.Y.1,
    k=as.numeric(clean.file$Final)-1,
    full.name=clean.file$Name,
    assay.name=clean.file$Assay
    )
  
  core.data <- arrange(core.data, assay, name)
  core.data
}


recall.df <- ReOrganizeFile(combn.recall.FILE)

# Since the fluidigm only allow things to be labeled into three groups (four if using "invalid" label), for loci with more than three or four clusters, I reuse the same label in full rotation. I need to correct the k value to match up the actual cluster ID. For now the max k:8

SpanningK <- function(orig.k ,rel.dye1, rel.dye2) 
{
  k <- orig.k
    if(max(k) > 3) {
      sort.k.ind <- order(desc(atan(rel.dye2/rel.dye1)))
      sort.k <- orig.k[sort.k.ind]
      
      sort.k.gt0.ind <- sort.k.ind[which(sort.k>0)]
      sort.k.gt0 <- orig.k[sort.k.gt0.ind]
      
      track.counter <- 0
      inc.switch <- 0
      for ( i in 2:length(sort.k.gt0)) {
        if(sort.k.gt0[i-1] - sort.k.gt0[i] >= 2) {
          inc.switch <- 1
          track.counter <- sort.k.gt0[i]
        }
        
        if (inc.switch == 1 && abs(sort.k.gt0[i] - track.counter) %in% c(0,1)) {
          k[sort.k.gt0.ind[i]] <- k[sort.k.gt0.ind[i]] + 4
          track.counter <- max(sort.k.gt0[i], track.counter)
        }
      }}
    k
}

recall.k.df <- recall.df %>%
  group_by(assay) %>%
  mutate(new.k = SpanningK(k, rel.dye1, rel.dye2)) %>%
  mutate(total.k = max(new.k))
         

recall.k.df %>%
  group_by(assay) %>%
  summarise(total.k = max(new.k)) %>%
  View()

region.FILE=read.csv("regional.tbl")
colnames(region.FILE) <- c("full.name", "region")

recall.k.region <- inner_join(recall.k.df, region.FILE)

# reduce any duplicated samples into one (Pick the best one if k>0, highest sum signal)

reduce.data <- recall.k.region %>% 
  group_by(assay, name) %>% 
  arrange(.,desc(k>0), desc(rel.dye1+rel.dye2)) %>% 
  filter(row_number()==1)
  

north.obs <- reduce.data %>%
  filter(region %in% "North") %>%
  group_by(assay.name) %>%
  summarise(ct.0=sum(new.k==0),
            ct.1=sum(new.k==1),
            ct.2=sum(new.k==2),
            ct.3=sum(new.k==3),
            ct.4=sum(new.k==4),
            ct.5=sum(new.k==5),
            max.k =max(total.k) )


south.obs <- reduce.data %>%
  filter(region %in% "South") %>%
  group_by(assay.name) %>%
  summarise(ct.0=sum(new.k==0),
            ct.1=sum(new.k==1),
            ct.2=sum(new.k==2),
            ct.3=sum(new.k==3),
            ct.4=sum(new.k==4),
            ct.5=sum(new.k==5),
            max.k =max(total.k))


```


```{r}
cal.pred.one.y <- function(data, data.rm){

  y.tbl <- as.matrix(data[,3:7])-data.rm
  okay.loci <- data$max.k > 0
  occupy.matrix <- okay.loci*t(sapply(1:nrow(data), function(i){
    blank<-rep(0,5)
    blank[1:data$max.k[i]] <- 1
    blank
  }))
  
  #hyperparam value : for now, assume uniform
  hyper.alpha <- ifelse(data$max.k >0, 1/data$max.k, 0)
  
  #data$max.
  new.alpha<- (hyper.alpha + y.tbl)*occupy.matrix
  
  ## This is the long analytical way
  
  #lg.new.alpha <- lgamma(new.alpha)
  #lg.new.alpha[occupy.matrix==0]<-0
  #p1<- rowSums(lg.new.alpha)
  #p2 <- lgamma(rowSums(new.alpha))
  #p2[okay.loci==0]<-0
  #p3 <- lgamma(rowSums(new.alpha)+1)
  #p4 <- (lgamma(new.alpha+1)+p1-lg.new.alpha)*occupy.matrix  
  #exp((p4+(p2-p1-p3))*occupy.matrix)
  
  # the quicker way is this
  prob.new <- new.alpha/rowSums(new.alpha)
  prob.new[occupy.matrix==0]<-0
  prob.new
}


get.regional.prob <- function(assay, new.k, region, for.region="North", take.one.out=F) {
  
  n.loci<- dim(prob.matrix.north)[1]
  n.k <- dim(prob.matrix.north)[2]
  region <- region[1]
  
  rm.matrix<-matrix(0, ncol=n.k, nrow=n.loci)
  
  if(take.one.out && for.region==region) {
    rm.matrix[cbind(assay,new.k)] <- 1
  }
  
  if(for.region=="North") {
       prob.matrix.north <- cal.pred.one.y(north.obs, data.rm<-rm.matrix)
    exp(sum(log(prob.matrix.north[cbind(assay, new.k)])))
  }
  else {
    prob.matrix.south <- cal.pred.one.y(south.obs, data.rm<-rm.matrix)
    exp(sum(log(prob.matrix.south[cbind(assay, new.k)])))
  }
}

#Take one North data out
reduce.data %>%
  filter(region %in% "North",total.k > 0) %>%
  arrange(name, assay) %>%
  group_by(name) %>%
  summarise(prob.north =get.regional.prob(assay, new.k, region, for.region="North", take.one.out=T), 
            prob.south =get.regional.prob(assay, new.k, region, for.region="South", take.one.out=T) ) %>%
  mutate(prob.being.north=prob.north/(prob.north+prob.south))


reduce.data %>%
  filter(region %in% "South",total.k > 0) %>%
  arrange(name, assay) %>%
  group_by(name) %>%
  summarise(prob.north =get.regional.prob(assay, new.k, region, for.region="North", take.one.out=T), 
            prob.south =get.regional.prob(assay, new.k, region, for.region="South", take.one.out=T) ) %>%
  mutate(prob.being.north=prob.north/(prob.north+prob.south))



region.unk.result <- reduce.data %>%
  filter(!(region %in% c("North","South")),total.k > 0) %>%
  arrange(name, assay) %>%
  group_by(name) %>%
  summarise(prob.north =get.regional.prob(assay, new.k, region, for.region="North"), 
            prob.south =get.regional.prob(assay, new.k, region, for.region="South") ) %>%
  mutate(prob=prob.north/(prob.north+prob.south))


reduce.data %>%
  filter(total.k > 0) %>%
  arrange(name) %>% group_by(name) %>% filter(new.k==0) %>%
  tally() %>%
  select(n) %>% 
  table


ggplot(reduce.data %>% filter(total.k > 0, new.k >0, region %in% c("North","South")), aes(x=assay, y=full.name, z=new.k, color=factor(new.k)))+
  facet_wrap(~region, ncol=1, drop=T, scale="free_y")+
  geom_point()+
  theme_bw()

# get chi-seq p value
p.val.NS <- apply(cbind(north.obs, south.obs) , 1 , function(x) {
    num.k <- as.numeric(x[16])
    chisq.test(cbind(1+as.numeric(x[3:(3+num.k-1)]),
                    1+as.numeric(x[11:(11+num.k-1)])))$p.val
    })

NS.table <- as.data.frame(rbind(cbind(north.obs, 
                                      region="North", 
                                      p.val = p.val.NS),
                                cbind(south.obs, 
                                      region="South", 
                                      p.val = p.val.NS)))
melt.table <- melt(NS.table,id.vars = c("assay.name", "region", "p.val")) %>% 
  filter(value>0, !(variable %in% c("ct.0", "max.k"))) 

melt.table$assay.name <- with(melt.table, reorder(assay.name, -p.val))

melt.table.frac.N <- melt.table %>% 
  filter(region=="North") %>% 
  mutate(fraction=value/21)

melt.table.frac.S <- melt.table %>% 
  filter(region=="South") %>% 
  mutate(fraction=value/39)

ggplot(rbind(melt.table.frac.N, melt.table.frac.S), aes(x=assay.name,y=fraction, fill=variable))+
  facet_wrap(~region)+
  geom_bar(stat = "identity",position = "stack")+
  coord_flip()+
  scale_fill_brewer(palette = "Set2", name="cluster #", labels=c(1,2,3,4,5))+
  ylab("Group fraction")+
  xlab("Locus Name - sort by Chi-seq value")

## make PCA plot

df.for.pca <-reduce.data %>% filter(total.k > 0) 

total.assay <- max(df.for.pca$assay)
total.name <- max(df.for.pca$name)

data.for.pca <- matrix(0, ncol=total.assay, nrow=total.name)
data.for.pca[cbind(df.for.pca$name, df.for.pca$assay)] <- df.for.pca$new.k

# remove all rows that are zeros
nonzero.row <- apply(data.for.pca, 1, sum) > 0
pca.raw <- data.for.pca[nonzero.row,]

pca <- prcomp(pca.raw, scale=F)


chisq.rval <- round(log(p.val.NS))
pca.melted <- melt(pca$rotation[,1:9])
pca.label <- cbind(pca.melted, chisq.rval)

pca.label$Var1 <- with(pca.label, reorder(Var1, -chisq.rval))

ggplot(data=pca.label) +
  geom_bar(aes(x=Var1, y=value, fill=factor(chisq.rval)), stat="identity") +
  facet_wrap(~Var2)

sample.groups <- reduce.data %>% group_by(name) %>% summarise(regional.labels=region[1]) %>% select(regional.labels)
scores <- data.frame(sample.groups, pca$x[,1:3])

ggplot(scores %>% filter(regional.labels %in% c("North","South")), 
       aes(x=PC1, y=PC2, colour=regional.labels))+
  geom_point(size=4, alpha=0.8)+
  scale_color_discrete("Regional Labels", breaks=c("North", "South", "BC3916", "BC4332"))+
  geom_point(data=scores %>% filter(!regional.labels %in% c("North","South")),
             aes(x=PC1, y=PC2, colour=regional.labels),
             size=2)+
  theme_bw()


ggplot(scores %>% filter(regional.labels %in% c("North","South")), 
       aes(x=PC1, y=PC2, colour=regional.labels))+
  geom_point(size=4, alpha=0.8)+
  scale_color_discrete("Regional Labels", breaks=c("North", "South", "BC3916", "BC4332"))+
  geom_point(data=scores %>% filter(!regional.labels %in% c("North","South")),
             aes(x=PC1, y=PC2, colour=regional.labels),
             size=2)+
  theme_bw()

ggplot(scores %>% filter(regional.labels %in% c("North","South")), 
       aes(x=PC1, y=PC3, colour=regional.labels))+
  geom_point(size=4, alpha=0.8)+
  scale_color_discrete("Regional Labels", breaks=c("North", "South", "BC3916", "BC4332"))+
  geom_point(data=scores %>% filter(!regional.labels %in% c("North","South")),
             aes(x=PC1, y=PC2, colour=regional.labels),
             size=2)+
  theme_bw()

ggplot(scores %>% filter(regional.labels %in% c("North","South")), 
       aes(x=PC2, y=PC3, colour=regional.labels))+
  geom_point(size=4, alpha=0.8)+
  scale_color_discrete("Regional Labels", breaks=c("North", "South", "BC3916", "BC4332"))+
  geom_point(data=scores %>% filter(!regional.labels %in% c("North","South")),
             aes(x=PC1, y=PC2, colour=regional.labels),
             size=2)+
  theme_bw()


```





Might need to enforce a minimal signal 






# Supervised learning 
  
 * posterior cluster assginment based on NN-cluster
 * based on cluster-centroid
 * based on smooth unspecificed kernel density
 * based on MVN 



- module for make

```
library(scales)
reverselog_trans <- function(base = exp(1)) {
    trans <- function(x) -log(x, base)
    inv <- function(x) base^(-x)
    trans_new(paste0("reverselog-", format(base)), trans, inv, 
              log_breaks(base = base), 
              domain = c(1e-100, Inf))
}
```
